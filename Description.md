# Score Description Page

## Preliminary
We assume the input of each layer in the shape of `C x H x W` where `C` is the channel number of the input feature map, 
`H` is the height of the input feature map and `W`
is the width of the input feature map. We define the param in convolution layers in the shape of `n x c x h x w` where `n` is the kernel 
number, i.e., the output channel number 
of the output feature map, `c` in the input channel number, `h` is the height of each kernel and `w` is the width of each kernel.

## Convolution and inner-product layer :
* Op :
  1. Accumulation : 
    * Adder Tree :
    We use adder tree to accumulate the intermediate results generated by the multiplication between low-bit int/uint activation and low-bit int weights. Note that only non-zero weights will be taken into account when accumulate intermediate results. During the accumulating, we use different bit-widths to make sure that no overflow will occur in the addition, changing along with the depth of the adder tree. Please refer to our [accumulation Demo](xxxx). 
    * Fixed-length integer accumulation : 
    If adder tree is not allowd, we use the fixed-length integer to accumulate the intermediate results. The fixed-length for accumulation is selected to make assure that there is no overflow during the accumulating process. We detail the selection of fixed-length in our [code](https://github.com/wps712/MicroNetChallenge/blob/754593e4a7a57231ae27375117ea5b9325e75280/flops_utils_final.py#L165).
  2. Multiplication : We use `min(act_bitwidth, weight_bitwidth)` as the mul_bit_base.
  3. First convolution layer :
  We merge the mean and variance terms in the data layer into the first scale layer, thus the input of the network is fully uint8.
  4. Potential overflow & Precision mismatch :
  We merge the mean and variance terms in the data layer into the first scale layer, thus the input of the network is fully uint8.
     
* Param : 
  


## Scale layer :
1. Op :
  
2. Param :
  
  
## ReLU layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L260)
* Param :
  * no param

## Pooling layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L270)
* Param :
  * no param
  
## Quantization layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L243)
* Param :
  * Type : FP16 (owing to `model.half()`)
  * If alpha equals to one, then there is no param in this quantization layer, otherwise, the param number is `1`.
  
## Sigmoid layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L311)
* Param :
  * no param
  
## Swish layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L321)
* Param :
  * no param
  
## Point Add layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L342)
* Param :
  * no param
  
## Point Dot layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L331)
* Param :
  * no param
