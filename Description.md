# Score Description Page

## Preliminary
We assume the input of each layer in the shape of `C x H x W` where `C` is the channel number of the input feature map, 
`H` is the height of the input feature map and `W`
is the width of the input feature map. We define the param in convolution layers in the shape of `n x c x h x w` where `n` is the kernel 
number, i.e., the output channel number 
of the output feature map, `c` in the input channel number, `h` is the height of each kernel and `w` is the width of each kernel.

We merge the scale layer into the convolution and inner-product layers:

![equation](http://latex.codecogs.com/gif.latex?y=k(W*x)+b)

![equation](http://latex.codecogs.com/gif.latex?y=k(Wx)+b)

where ![equation](http://latex.codecogs.com/gif.latex?*) refers to the convolution operation.

## Convolution and inner-product layer :
* Op :
  1. Accumulation : 
    * Adder Tree :
    We use adder tree to accumulate the intermediate results generated by the multiplication between low-bit int/uint activation and low-bit int weights. Note that only non-zero weights will be taken into account when accumulate intermediate results. During the accumulating, we use different bit-widths to make sure that no overflow will occur in the addition, changing along with the depth of the adder tree going deeper. Please refer to our [accumulation Demo](xxxx). 
    * Fixed-length accumulation : 
    If adder tree is not allowd, we use the fixed-length integer to accumulate the intermediate results. The fixed-length for accumulation is selected to make assure that there is no overflow during the accumulating process. We detail the selection of fixed-length in our [code](https://github.com/wps712/MicroNetChallenge/blob/754593e4a7a57231ae27375117ea5b9325e75280/flops_utils_final.py#L165).
  2. Multiplication : We use `min(act_bitwidth, weight_bitwidth)` as the mul_bit_base.
  3. First convolution layer :
  We merge the mean and variance terms in the data layer into the first scale layer, thus the input of the network is fully uint8.
  4. Potential overflow & Precision mismatch :
  Note that accumulation using **Adder Tree** and **Fixed-length accumulation** will not run out of the range of the FP32, therefore, we can freely convert the results into FP32. However, we did compute other parts of the networks in FP16 since model.half was used, the conversion between float to half is unsafe. To solve this problem, we run convolution layers and inner-product layers in FP32, then convert the output to FP16 and run other parts in FP16, and test the whole model on validation set to verify that the Top-1 accuracy is indeed higher than 75%. For simplicity, we use the original FP32 convolution operations to simulate low-bit multiplication with **Adder Tree**/**Fixed-length accumulation** accumulations (note that **Adder Tree** and **Fixed-length accumulation** will not run out of the range of the FP32).
     
* Param : 
  * Type : FP16 (owing to `model.half()`)
  * We use the same [param counting](https://github.com/wps712/MicroNetChallenge/blob/754593e4a7a57231ae27375117ea5b9325e75280/flops_utils_final.py#L47) in all the above cases.


## Scale layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/754593e4a7a57231ae27375117ea5b9325e75280/flops_utils_final.py#L46)
* Param :
  * The param number is `2C`
  
  
## ReLU layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L260)
* Param :
  * no param

## Pooling layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L270)
* Param :
  * no param
  
## Quantization layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L243)
* Param :
  * Type : FP16 (owing to `model.half()`)
  * If alpha equals to one, then there is no param in this quantization layer, otherwise, the param number is `1`.
  
## Sigmoid layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L311)
* Param :
  * no param
  
## Swish layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L321)
* Param :
  * no param
  
## Point Add layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L342)
* Param :
  * no param
  
## Point Dot layer :
* Op :
  * Type : FP16 (owing to `model.half()` and `x.half()`)
  * Count : [Link](https://github.com/wps712/MicroNetChallenge/blob/fa136d792419d236c28137bdea48f498dda49fad/flops_utils_final.py#L331)
* Param :
  * no param
